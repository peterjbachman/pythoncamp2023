{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Day 4: Web Scraping + File I/O\n",
    "\n",
    "Instructions: \n",
    "\n",
    "1. Go to https://polisci.wustl.edu/people/88/all OR https://polisci.wustl.edu/people/list/88/all\n",
    "2. Go to the page for each of the professors.\n",
    "3. Create a `.csv`` file with the following information for each professor:\n",
    "\t- Name\n",
    "\t- Title\n",
    "\t- E-mail\n",
    "\t- Web page\n",
    "\t- Specialization  \n",
    "\t\t- If they do not have a specialization, you can leave it blank. \n",
    "\t\t- An example from Deniz's page: https://polisci.wustl.edu/people/deniz-aksoy\n",
    "\t\t- Professor Aksoyâ€™s research is motivated by an interest in comparative political institutions and political violence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/ysui/Desktop/PhD/MTE/pythoncamp2023_prep/Day04/Lab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on 0 of 35:\n",
      "Working on 1 of 35:\n",
      "Working on 2 of 35:\n",
      "Working on 3 of 35:\n",
      "Working on 4 of 35:\n",
      "Working on 5 of 35:\n",
      "Working on 6 of 35:\n",
      "Working on 7 of 35:\n",
      "Working on 8 of 35:\n",
      "Working on 9 of 35:\n",
      "Working on 10 of 35:\n",
      "Working on 11 of 35:\n",
      "Working on 12 of 35:\n",
      "Working on 13 of 35:\n",
      "Working on 14 of 35:\n",
      "Working on 15 of 35:\n",
      "Working on 16 of 35:\n",
      "Working on 17 of 35:\n",
      "Working on 18 of 35:\n",
      "Working on 19 of 35:\n",
      "Working on 20 of 35:\n",
      "Working on 21 of 35:\n",
      "Working on 22 of 35:\n",
      "Working on 23 of 35:\n",
      "Working on 24 of 35:\n",
      "Working on 25 of 35:\n",
      "Working on 26 of 35:\n",
      "Working on 27 of 35:\n",
      "Working on 28 of 35:\n",
      "Working on 29 of 35:\n",
      "Working on 30 of 35:\n",
      "Working on 31 of 35:\n",
      "Working on 32 of 35:\n",
      "Working on 33 of 35:\n",
      "Working on 34 of 35:\n",
      "Working on 35 of 35:\n",
      "All done!\n"
     ]
    }
   ],
   "source": [
    "with open('lab04.csv', 'w') as f: # open new csv file\n",
    "    w = csv.DictWriter(f, fieldnames = (\"name\", \"title\", \"email\", \"website\", \"specialization\")) # set colnames\n",
    "    w.writeheader() # write header\n",
    "\n",
    "\t# set up selenium stuff\n",
    "    driver_path = Service('/Users/ysui/Desktop/PhD/MTE/pythoncamp2023_prep/Day04/Lecture/chromedriver')\n",
    "\n",
    "\t# open webpage\n",
    "    driver = webdriver.Chrome(service = driver_path)\n",
    "    driver.get('https://polisci.wustl.edu/people/88/all') # go to website\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # scroll to bottom\n",
    "    time.sleep(5) # pause to let selenium catch up and actually scroll\n",
    "    html = driver.page_source # get html\n",
    "    driver.close() # close selenium browser\n",
    "    soup = BeautifulSoup(html) # soup html\n",
    "\n",
    "    cards = soup.find_all('a', {'class' : 'card'}) # get all faculty\n",
    "\n",
    "    for c in range(len(cards)+1): # for each faculty card\n",
    "        print(\"Working on \" + str(c) + \" of \" + str(len(cards)) + \":\")\n",
    "        try:\n",
    "            fac = {} # empty dict\n",
    "            fac['name'] = ' '.join(cards[c].find('h3').text.split('\\xa0')) # get name, split on weird encoding and re-join\n",
    "            fac['title'] = cards[c].find('div', {'class' : 'dept'}).text # get title\n",
    "\n",
    "            interior = 'https://polisci.wustl.edu' + cards[c]['href'] # go to interior page\n",
    "            interior_page = urllib.request.urlopen(interior) # open interior\n",
    "            interior_soup = BeautifulSoup(interior_page.read()) # soup interior\n",
    "\n",
    "            fac['email'] = interior_soup.find('ul', {'class' : 'detail contact'}).find('a').text # get email\n",
    "            fac['website'] = interior_soup.find('ul', {'class' : 'links'}).find('a')['href'] # get personal website\n",
    "            fac['specialization'] = interior_soup.find('div', {'class' : 'post-excerpt'}).text # get specialization\n",
    "            w.writerow(fac) # write row\n",
    "        except:\n",
    "            continue # skip row on issue (e.g., Lee Epstien) # NOT the best way to deal with this. \n",
    "\n",
    "        time.sleep(3) # polite sleep\n",
    "\n",
    "print(\"All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
